[Last week](http://www.codecademy.com/groups/python-fro-beginners/discussions/52157d8a80ff33d70b001140), we discussed Big O notation and how it will help us analyze algorithms, and over the last several weeks, you’ve been given different sorting algorithms as problems of the week. Today we’re going to discuss the complexity of these algorithms at a high level.

**Insertion Sort**
Insertion sort goes through your first list and creates a second list inserting one element at a time such that each item is in order. The nice part about insertion sort is that if your list is already sorted, it does not have to make any swaps although it has to iterate the list once. Hence, in the best case, insertion sort has `O(n)` because it has `n` iterations and 0/constant swaps. But what happens if we have larger data that isn’t already sorted? In this case, we have to iterate our first list and for each item we potentially have to iterate the entire second list to find it’s proper place and do the required operations to insert it into it’s proper place which could cause a full iteration of the list. Hence, in the worst case, we have to iterate our list twice, i.e. `O(n**2)` with `n**2` comparisons and swaps. So, insertion sort is great for small, mostly sorted data but probably isn’t our algorithm of choice for larger data due to the polynomial complexity.

**Selection Sort**
In selection sort, you iterate the list finding the smallest element and moving it to the position from which you started iterating. It does this until all the elements are sorted. So, selection sort has to run through the list `n` times for `n` elements. Hence, we have another case of a `O(n**2)`. The nice part about selection sort is its Big Oh is the same for any given input and that’s it’s an inplace algorithm. So it has a space complexity of `O(1)` where insertion sort’s space complexity is `O(n)` with an additional temporary variable. So, you give up speed for space with selection sort whereas you give up space for potential speed in insertion sort. 

**Bubble Sort**
Bubble sort requires you to iterate the list and perform swaps until no swaps are required. This is nice because if your list is sorted, you iterate once giving yourself a `O(n)`. However, if the data is really mixed and matched, you have to run through the list similarly to what you did with selection sort. Hence, we have another sorting algorithm with time complexity of `O(n**2)` in the worst case scenario. Bubble sort is like selection sort in that its space requirement is also constant with an additional auxiliary variable. This is our third `O(n**2)` sorting algorithm. Is there a way to improve this?

**Merge Sort**
Merge sort is a *divide and conquer* algorithm. It takes our problem and splits it up into smaller problems that are easier to solve. Merge sort takes a list and continually splits it in half until the lists only have 1 element. Then, it combines these smaller lists so that they are stored in larger, sorted lists. The process to break the lists into halves only takes `log(n)` operations (where log is log base 2). The reason it is `log(n)` is because we only consider the subproblem we’re working on. When we split the list in half, we remove half of the set and we reduce our problem by a power of 2 or `log(n)`. In the next step, we combine the elements from these sublists. The lists are compared element by element and combined into a new list. This process requires to iterate the length of the new list or `n`. Hence, a subproblem of merge sort takes `n*log(n)` time. Thus, merge sort has `O(n*log(n))`. There are some outlying cases which we won’t cover that can cause higher times but these are rare. Merge sort is also nice because it only requires enough extra memory to contain the list you started with so the space complexity is `O(n)`.

**Quick Sort**
Without going into great detail about quick sort, I’m just going to tell you that this algorithm has time complexity of `O(n*log(n))` and space complexity of `O(n)` with potential for space complexity of `O(log(n))`. This algorithm is much more complex than we should cover in a beginners group. It’s a beautiful and elegant algorithm and I’ll pass you to Tim Roughgarden, professor at Stanford, for a great lecture series on quick sort. You find the videos [here](https://www.youtube.com/watch?v=TEkdSihDQIs). It’s a great set of lectures and I highly recommend them.

So we learned this week that there are three sorting algorithms with time complexity `O(n**2)` and two with complexity `O(n*log(n))`. Rather than just settling for naive solutions, we should always look for better solutions to problems which means analyzing the situation and determining the best course of action. I hope you enjoyed. See you next week. :)